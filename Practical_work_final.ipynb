{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41789,"status":"ok","timestamp":1721832032813,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"},"user_tz":-120},"id":"mXWWTg7lfuUD","outputId":"f8edad6e-df84-4280-94cf-ca4019f68758"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# import access to Google Drive files\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4015,"status":"ok","timestamp":1721832040774,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"},"user_tz":-120},"id":"6QQ0W2yUgCrr"},"outputs":[],"source":["# import needed libraries\n","\n","import os\n","import torch\n","import sys\n","import pandas as pd\n","import importlib"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4622,"status":"ok","timestamp":1721831587229,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"},"user_tz":-120},"id":"csZJSJ4VMuPS","outputId":"01d79abf-6804-4e4b-a276-5934e4ec04fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Pre-processing dataset for Ex2Vec...\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/JKU/practical_work/Practical-Work-AI/preprocess.py\", line 26, in <module>\n","    df = pd.read_csv(orig_dataset)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 912, in read_csv\n","    return _read(filepath_or_buffer, kwds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 577, in _read\n","    parser = TextFileReader(filepath_or_buffer, **kwds)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1407, in __init__\n","    self._engine = self._make_engine(f, self.engine)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\", line 1679, in _make_engine\n","    return mapping[engine](f, **self.options)\n","  File \"/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\", line 93, in __init__\n","    self._reader = parsers.TextReader(src, **kwds)\n","  File \"pandas/_libs/parsers.pyx\", line 550, in pandas._libs.parsers.TextReader.__cinit__\n","  File \"pandas/_libs/parsers.pyx\", line 639, in pandas._libs.parsers.TextReader._get_header\n","  File \"pandas/_libs/parsers.pyx\", line 850, in pandas._libs.parsers.TextReader._tokenize_rows\n","  File \"pandas/_libs/parsers.pyx\", line 861, in pandas._libs.parsers.TextReader._check_tokenize_status\n","  File \"pandas/_libs/parsers.pyx\", line 2029, in pandas._libs.parsers.raise_parser_error\n","pandas.errors.ParserError: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n"]}],"source":["# create train-val-test sets for Ex2Vec training, as well as sequences for GRU4Rec training\n","!python /content/drive/MyDrive/JKU/practical_work/Practical-Work-AI/preprocess.py"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6156,"status":"ok","timestamp":1721832049613,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"},"user_tz":-120},"id":"_8Jv9e4Jf7OF","outputId":"386c833f-1e18-4629-f735-3a3d43db70df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tensorboardX\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.1)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n","Installing collected packages: tensorboardX\n","Successfully installed tensorboardX-2.6.2.2\n"]}],"source":["!pip install tensorboardX"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wfajT0rHgRIM","executionInfo":{"status":"ok","timestamp":1721832087987,"user_tz":-120,"elapsed":38388,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"28cba220-1afa-4ca6-cd71-a60fe6f84b88"},"outputs":[{"output_type":"stream","name":"stdout","text":["The size of the training set is: 1106989\n","The size of the validation set is: 156748\n","The size of the test set is: 320078\n"]}],"source":["# import custom code\n","\n","# Append the directory containing 'data_sampler' and 'ex2vec' to Python's search path\n","sys.path.append('/content/drive/MyDrive/JKU/practical_work/Practical-Work-AI')\n","\n","# imports modules for preparing data and for training/evaluating the ex2vec model\n","import data_sampler\n","from ex2vec import Ex2VecEngine\n","\n","#import dir of gru4rec_pytorch module to python path in order to be able to access GRU4Rec model class to be able to load it\n","sys.path.append('/content/drive/MyDrive/JKU/practical_work/Practical-Work-AI/GRU4Rec_PyTorch_Fork')\n","from gru4rec_pytorch import GRU4RecModel\n","import evaluation as GRUeval"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1k-KR2WPgCCV","executionInfo":{"status":"ok","timestamp":1721832087988,"user_tz":-120,"elapsed":63,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"50f831ea-d4ac-4b8f-9fe2-995af0bdb4d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using GPU: Tesla T4\n","Current device: cuda\n"]}],"source":["# Check if gpu is available\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","    device_name = torch.cuda.get_device_name(device)\n","    print(f'Using GPU: {device_name}')\n","else:\n","    device = torch.device('cpu')\n","    print('Using CPU')\n","\n","print(f'Current device: {device}')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"XC3obowMLa3x","executionInfo":{"status":"ok","timestamp":1721832087989,"user_tz":-120,"elapsed":59,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}}},"outputs":[],"source":["# check that deezer parameter file exists\n","param_file_path = '/content/drive/MyDrive/JKU/practical_work/Practical-Work-AI/GRU4Rec_PyTorch_Fork/paramfiles/deezer_paramfile.py'\n","assert os.path.isfile(param_file_path), f'Parameter file not found at {param_file_path}'"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"yy0ssBG2P9_M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721832638745,"user_tz":-120,"elapsed":356744,"user":{"displayName":"Laura Legat","userId":"08447403520972321571"}},"outputId":"f6a75fdb-1897-4d82-ae40-73c8f775357f"},"outputs":[{"output_type":"stream","name":"stdout","text":["The size of the training set is: 1106989\n","The size of the validation set is: 156748\n","The size of the test set is: 320078\n","Ex2Vec(\n","  (user_lamb): Embedding(3623, 1)\n","  (user_bias): Embedding(3623, 1)\n","  (item_bias): Embedding(879, 1)\n","  (embedding_user): Embedding(3623, 64)\n","  (embedding_item): Embedding(879, 64)\n","  (logistic): Sigmoid()\n",")\n","global_lamb <class 'torch.Tensor'> torch.Size([])\n","alpha <class 'torch.Tensor'> torch.Size([])\n","beta <class 'torch.Tensor'> torch.Size([])\n","gamma <class 'torch.Tensor'> torch.Size([])\n","cutoff <class 'torch.Tensor'> torch.Size([])\n","user_lamb.weight <class 'torch.Tensor'> torch.Size([3623, 1])\n","user_bias.weight <class 'torch.Tensor'> torch.Size([3623, 1])\n","item_bias.weight <class 'torch.Tensor'> torch.Size([879, 1])\n","embedding_user.weight <class 'torch.Tensor'> torch.Size([3623, 64])\n","embedding_item.weight <class 'torch.Tensor'> torch.Size([879, 64])\n","Using validation set for evaluation\n","\n","started training model:  ex2vec_BS512LR5e-05L_DIM64\n","Epoch 0 starts !\n","100% 2163/2163 [00:23<00:00, 93.53it/s]\n","[Evaluating Epoch 0] ACC = 0.4744, B_ACC = 0.5000, RECALL = 0.0000, F1 = 0.3053\n","Epoch 1 starts !\n","100% 2163/2163 [00:21<00:00, 102.75it/s]\n","[Evaluating Epoch 1] ACC = 0.5212, B_ACC = 0.5144, RECALL = 0.6461, F1 = 0.5129\n","Epoch 2 starts !\n","100% 2163/2163 [00:21<00:00, 99.42it/s] \n","[Evaluating Epoch 2] ACC = 0.5245, B_ACC = 0.5173, RECALL = 0.6583, F1 = 0.5150\n","Epoch 3 starts !\n","100% 2163/2163 [00:21<00:00, 100.51it/s]\n","[Evaluating Epoch 3] ACC = 0.5282, B_ACC = 0.5217, RECALL = 0.6481, F1 = 0.5207\n","Epoch 4 starts !\n","100% 2163/2163 [00:21<00:00, 100.52it/s]\n","[Evaluating Epoch 4] ACC = 0.5315, B_ACC = 0.5239, RECALL = 0.6724, F1 = 0.5210\n","Epoch 5 starts !\n","100% 2163/2163 [00:21<00:00, 100.76it/s]\n","[Evaluating Epoch 5] ACC = 0.5348, B_ACC = 0.5275, RECALL = 0.6717, F1 = 0.5250\n","Epoch 6 starts !\n","100% 2163/2163 [00:21<00:00, 100.93it/s]\n","[Evaluating Epoch 6] ACC = 0.5380, B_ACC = 0.5295, RECALL = 0.6955, F1 = 0.5249\n","Epoch 7 starts !\n","100% 2163/2163 [00:21<00:00, 99.44it/s] \n","[Evaluating Epoch 7] ACC = 0.5418, B_ACC = 0.5327, RECALL = 0.7099, F1 = 0.5268\n","Epoch 8 starts !\n","100% 2163/2163 [00:21<00:00, 99.92it/s]\n","[Evaluating Epoch 8] ACC = 0.5465, B_ACC = 0.5378, RECALL = 0.7077, F1 = 0.5329\n","Epoch 9 starts !\n","100% 2163/2163 [00:21<00:00, 101.33it/s]\n","[Evaluating Epoch 9] ACC = 0.5500, B_ACC = 0.5412, RECALL = 0.7120, F1 = 0.5363\n","Saving model at epoch  9\n","Saving to  /content/drive/MyDrive/JKU/practical_work/Practical-Work-AI/checkpoints/ex2vec_BS512LR5e-05L_DIM64_Epoch9_f10.5363.pt\n","Epoch 10 starts !\n","100% 2163/2163 [00:21<00:00, 101.16it/s]\n","[Evaluating Epoch 10] ACC = 0.5529, B_ACC = 0.5431, RECALL = 0.7350, F1 = 0.5355\n","Epoch 11 starts !\n","100% 2163/2163 [00:21<00:00, 101.50it/s]\n","[Evaluating Epoch 11] ACC = 0.5557, B_ACC = 0.5465, RECALL = 0.7272, F1 = 0.5405\n","Epoch 12 starts !\n","100% 2163/2163 [00:21<00:00, 99.75it/s]\n","[Evaluating Epoch 12] ACC = 0.5575, B_ACC = 0.5478, RECALL = 0.7365, F1 = 0.5408\n","Epoch 13 starts !\n","100% 2163/2163 [00:21<00:00, 101.86it/s]\n","[Evaluating Epoch 13] ACC = 0.5619, B_ACC = 0.5553, RECALL = 0.6855, F1 = 0.5543\n","Epoch 14 starts !\n","100% 2163/2163 [00:21<00:00, 102.35it/s]\n","[Evaluating Epoch 14] ACC = 0.5660, B_ACC = 0.5580, RECALL = 0.7134, F1 = 0.5550\n","Epoch 15 starts !\n","100% 2163/2163 [00:21<00:00, 101.78it/s]\n","[Evaluating Epoch 15] ACC = 0.5705, B_ACC = 0.5625, RECALL = 0.7191, F1 = 0.5594\n","Epoch 16 starts !\n","100% 2163/2163 [00:21<00:00, 102.03it/s]\n","[Evaluating Epoch 16] ACC = 0.5765, B_ACC = 0.5680, RECALL = 0.7331, F1 = 0.5642\n","Epoch 17 starts !\n","100% 2163/2163 [00:21<00:00, 102.01it/s]\n","[Evaluating Epoch 17] ACC = 0.5823, B_ACC = 0.5736, RECALL = 0.7433, F1 = 0.5695\n","Epoch 18 starts !\n","100% 2163/2163 [00:21<00:00, 100.06it/s]\n","[Evaluating Epoch 18] ACC = 0.5861, B_ACC = 0.5769, RECALL = 0.7566, F1 = 0.5718\n","Epoch 19 starts !\n","100% 2163/2163 [00:21<00:00, 101.08it/s]\n","[Evaluating Epoch 19] ACC = 0.5900, B_ACC = 0.5800, RECALL = 0.7738, F1 = 0.5734\n","Saving model at epoch  19\n","Saving to  /content/drive/MyDrive/JKU/practical_work/Practical-Work-AI/checkpoints/ex2vec_BS512LR5e-05L_DIM64_Epoch19_f10.5734.pt\n","Epoch 20 starts !\n","100% 2163/2163 [00:21<00:00, 100.27it/s]\n","[Evaluating Epoch 20] ACC = 0.5940, B_ACC = 0.5833, RECALL = 0.7912, F1 = 0.5749\n","Epoch 21 starts !\n","100% 2163/2163 [00:21<00:00, 99.27it/s] \n","[Evaluating Epoch 21] ACC = 0.5981, B_ACC = 0.5871, RECALL = 0.8021, F1 = 0.5778\n","Epoch 22 starts !\n"," 15% 314/2163 [00:03<00:18, 102.68it/s]\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/JKU/practical_work/Practical-Work-AI/train.py\", line 63, in <module>\n","    engine.train_an_epoch(train_loader, epoch_id=epoch, embds_path=args.embds_path) # train 1 epoch\n","  File \"/content/drive/MyDrive/JKU/practical_work/Practical-Work-AI/engine.py\", line 55, in train_an_epoch\n","    for batch_id, batch in tqdm(enumerate(train_loader),total=len(train_loader)):\n","  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1181, in __iter__\n","    for obj in iterable:\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n","    data = self._next_data()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 675, in _next_data\n","    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n","    return self.collate_fn(data)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 316, in default_collate\n","    return collate(batch, collate_fn_map=default_collate_fn_map)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 173, in collate\n","    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 173, in <listcomp>\n","    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 141, in collate\n","    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 213, in collate_tensor_fn\n","    return torch.stack(batch, 0, out=out)\n","KeyboardInterrupt\n"]}],"source":["# train Ex2Vec\n","!python /content/drive/MyDrive/JKU/practical_work/Practical-Work-AI/train.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MTRuguVwPtI-"},"outputs":[],"source":["# train GRU4Rec\n","\n","\"\"\"\n","params: (from https://github.com/hidasib/GRU4Rec/blob/master/README.md)\n","  -t    Testset path\n","  -pf   Parameter file path\n","  -s    Path to save the state dict to\n","  -m    Calculate recall, MRR etc. at the given list length\n","  -ik   Item key\n","  -tk   Timestamp key\n","  -d    Device\n","\"\"\"\n","\n","!python /content/drive/MyDrive/JKU/practical_work/GRU4Rec/run.py /content/drive/MyDrive/JKU/practical_work/Practical-Work-AI/data/seq_train.csv -t /content/drive/MyDrive/JKU/practical_work/Practical-Work-AI/data/seq_val.csv -pf /content/drive/MyDrive/JKU/practical_work/Practical-Work-AI/GRU4Rec_PyTorch_Fork/paramfiles/deezer_paramfile.py -s /content/drive/MyDrive/JKU/practical_work/Practical-Work-AI/models/GRU4Rec.pt -m 1 5 10 20 -ik \"itemId\" -tk \"timestamp\" -d cpu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vo-HfJyEPwid"},"outputs":[],"source":["# extract item embeddings from GRU4Rec Model\n","\n","model_loaded = torch.load('/content/drive/MyDrive/JKU/practical_work/Practical-Work-AI/models/GRU4Rec.pt')\n","item_embds = model_loaded.model.Wy.weight.data\n","item_embds.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XaD31WfOPz-1"},"outputs":[],"source":["# retrain Ex2Vec with GRU4Rec item embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cKDD3P4mQFQ-"},"outputs":[],"source":["# compare performance"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyP41aCYeNtYlyn5s65Gedth"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}